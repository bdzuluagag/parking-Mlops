---

# üÖøÔ∏è Predicci√≥n de Ocupaci√≥n de Parqueaderos con GCP y Vertex AI

**Autores:**

* Brayan David Zuluaga Giraldo ‚Äî [bdzuluagag@eafit.edu.co](mailto:bdzuluagag@eafit.edu.co)
* Sof√≠a Mendieta Mar√≠n ‚Äî [smendietam@eafit.edu.co](mailto:smendietam@eafit.edu.co)

**Curso:** ST1630 - Reto 4 / Proyecto Final
**Instituci√≥n:** Universidad EAFIT
**A√±o:** 2025

---

## üìò Descripci√≥n del Caso

### üéØ Caso de negocio

El crecimiento del parque automotor en las principales ciudades de Colombia ha incrementado la demanda por soluciones inteligentes de movilidad.
Uno de los retos m√°s comunes en entornos urbanos es la **baja disponibilidad y localizaci√≥n ineficiente de espacios de parqueo**, lo que genera congesti√≥n y p√©rdidas econ√≥micas.

El presente proyecto busca **predecir la ocupaci√≥n futura de parqueaderos urbanos en Colombia** utilizando un flujo de datos en tiempo real y t√©cnicas de aprendizaje autom√°tico.

La **pregunta anal√≠tica principal** es:

> ‚Äú¬øPodemos anticipar cu√°ntos cupos libres tendr√° un parqueadero en los pr√≥ximos minutos bas√°ndonos en su comportamiento reciente?‚Äù

El modelo desarrollado permite estimar, con base en los datos hist√≥ricos de disponibilidad, **la cantidad de espacios disponibles dentro de los pr√≥ximos 10 intervalos de tiempo**, apoyando as√≠ la toma de decisiones en la gesti√≥n de movilidad urbana.

---

### üíª Caso tecnol√≥gico

El caso aborda un reto de **Big Data y MLOps**, centrado en la integraci√≥n de fuentes de datos en streaming, su procesamiento en la nube y el despliegue de un modelo de Machine Learning en un entorno gestionado.

El pipeline propuesto combina servicios de **Google Cloud Platform (GCP)** para construir una soluci√≥n escalable, automatizada y anal√≠tica, que permite:

* Ingestar datos de disponibilidad de parqueaderos en **tiempo real**.
* Procesarlos mediante un **pipeline de streaming con Dataflow (Apache Beam)**.
* Almacenarlos y transformarlos en **BigQuery**.
* Entrenar y desplegar un **modelo predictivo** con **Vertex AI (AutoML Tabular)**.
* Visualizar los resultados.

---

## üî¨ Metodolog√≠a Anal√≠tica ‚Äî CRISP-DM

El proyecto sigue la metodolog√≠a **CRISP-DM (Cross Industry Standard Process for Data Mining)**, que estructura el ciclo de vida anal√≠tico en seis fases:

| Etapa                           | Descripci√≥n                                                                                                                                                                                                                      |
| ------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **1. Comprensi√≥n del negocio**  | El objetivo es anticipar la ocupaci√≥n de parqueaderos para optimizar la gesti√≥n del tr√°fico y el aprovechamiento de la infraestructura en zonas urbanas de Colombia.                                                             |
| **2. Comprensi√≥n de los datos** | Los datos simulados representan sensores IoT que env√≠an, cada 5 segundos, informaci√≥n de disponibilidad (`timestamp`, `parking_id`, `available_spots`). Se transmiten en tiempo real a trav√©s de Pub/Sub.                        |
| **3. Preparaci√≥n de los datos** | Los datos son procesados con Dataflow y almacenados en BigQuery. Se construyeron variables derivadas (`lag_1`, `ma_5`, `target_t_plus_10`) mediante funciones de ventana SQL. Se eliminan valores nulos antes del entrenamiento. |
| **4. Modelado**                 | Se us√≥ Vertex AI AutoML Tabular con modelo de regresi√≥n supervisada. Variables de entrada: `current_value`, `lag_1`, `ma_5`, `parking_id`. Variable objetivo: `target_t_plus_10`.                                                |
| **5. Evaluaci√≥n**               | Vertex AI gener√≥ m√©tricas de desempe√±o (RMSE, MAE, R¬≤). El modelo predice con una precisi√≥n adecuada la disponibilidad futura.                                                                                                   |
| **6. Despliegue**               | El modelo fue desplegado en Vertex AI Endpoint para realizar predicciones en tiempo real mediante peticiones REST. Los resultados se visualizan en BigQuery Console.                                                             |

---

## ‚òÅÔ∏è Arquitectura de la Soluci√≥n (GCP)

| Etapa                       | Servicio GCP                         | Funci√≥n                                                                                                    |
| --------------------------- | ------------------------------------ | ---------------------------------------------------------------------------------------------------------- |
| **Ingesta (streaming)**     | **Pub/Sub**                          | Recibe datos de disponibilidad de parqueaderos en tiempo real (mensajes JSON simulados con `producer.py`). |
| **Procesamiento**           | **Dataflow (Apache Beam)**           | Limpia, transforma y carga los datos desde Pub/Sub hacia BigQuery.                                         |
| **Almacenamiento**          | **BigQuery**                         | Almacena los datos crudos (`raw_parking`) y las tablas derivadas (`training_data_clean`).                  |
| **Entrenamiento ML**        | **Vertex AI AutoML Tabular**         | Entrena el modelo de predicci√≥n de ocupaci√≥n de parqueaderos.                                              |
| **Evaluaci√≥n / Predicci√≥n** | **Vertex AI Endpoint**               | Permite realizar predicciones de disponibilidad futura v√≠a API.                                            |
| **Visualizaci√≥n**           | **BigQuery Console** | Permite consultas anal√≠ticas y visualizaci√≥n de resultados.                                                |

### üîß Diagrama general

```
[Producer.py] 
     ‚Üì 
[Pub/Sub Topic: parking-raw]
     ‚Üì 
[Dataflow Job ‚Üí BigQuery.raw_parking]
     ‚Üì 
[BigQuery.training_data_clean]
     ‚Üì 
[Vertex AI: Entrenamiento y despliegue de modelo]
     ‚Üì 
[Predicciones y visualizaci√≥n]
```

---

## ‚öôÔ∏è Implementaci√≥n T√©cnica

### a) Ingesta de datos (streaming)

* Simulaci√≥n mediante `producer.py`, que genera mensajes JSON cada 5 segundos.
* Publicaci√≥n en Pub/Sub (`parking-raw`).

### b) Procesamiento (Dataflow)

* `dataflow_pipeline.py` usa Apache Beam para leer desde Pub/Sub y escribir en BigQuery.
* Streaming habilitado en modo `DataflowRunner`.

### c) Almacenamiento y preparaci√≥n

* **BigQuery Dataset:** `parking_dataset`

  * `raw_parking`: datos crudos.
  * `training_data`: features y target generados con SQL.
  * `training_data_clean`: datos finales sin nulos.

Ejemplo de funciones SQL usadas:

```sql
LAG(available_spots, 1) OVER (PARTITION BY parking_id ORDER BY timestamp)
AVG(available_spots) OVER (PARTITION BY parking_id ORDER BY timestamp ROWS BETWEEN 4 PRECEDING AND CURRENT ROW)
LEAD(available_spots, 10) OVER (PARTITION BY parking_id ORDER BY timestamp)
```

### d) Entrenamiento (Vertex AI)

* **Tipo:** AutoML Tabular
* **Modelo:** Regresi√≥n
* **Target:** `target_t_plus_10`
* **Features:** `current_value`, `lag_1`, `ma_5`, `parking_id`
* **Evaluaci√≥n:** RMSE ‚âà 3.9, MAE ‚âà 2.1

### e) Despliegue y visualizaci√≥n

* Modelo desplegado en Vertex AI Endpoint.
* Consultas y reportes ejecutados directamente desde **BigQuery Console**.

---
---

## üöÄ Reproducci√≥n del Proyecto

### 1Ô∏è‚É£ Crear topic y suscripci√≥n

```bash
gcloud pubsub topics create parking-raw
gcloud pubsub subscriptions create parking-raw-sub --topic=parking-raw
```

### 2Ô∏è‚É£ Ejecutar productor

```bash
python3 producer.py
```

### 3Ô∏è‚É£ Ejecutar pipeline Dataflow

```bash
python3 dataflow_pipeline.py \
  --project=pure-heuristic-471315-i8 \
  --input_topic=projects/pure-heuristic-471315-i8/topics/parking-raw \
  --dataset_table=pure-heuristic-471315-i8:parking_dataset.raw_parking \
  --temp_location=gs://pure-heuristic-471315-i8-raw-data/temp \
  --region=us-central1 \
  --runner=DataflowRunner
```

### 4Ô∏è‚É£ Crear tablas derivadas en BigQuery

Ejecutar los SQL ubicados en la carpeta `sql/`.

### 5Ô∏è‚É£ Entrenar el modelo en Vertex AI

1. Crear dataset tabular desde BigQuery.
2. Definir `target_t_plus_10` como variable objetivo.
3. Entrenar con AutoML (regresi√≥n).

---

## üß≠ Conclusiones

* Se logr√≥ implementar un **pipeline completo de MLOps en GCP**, desde la ingesta hasta el despliegue del modelo.
* Los resultados muestran la **viabilidad de predecir la disponibilidad futura de parqueaderos** con datos en tiempo real.
* El uso de **servicios serverless (Pub/Sub, Dataflow, BigQuery, Vertex AI)** simplifica la infraestructura y permite escalar f√°cilmente.
* Este enfoque puede adaptarse a otros contextos urbanos en Colombia (tr√°nsito, transporte p√∫blico, estaciones de bicicletas, etc.).

---

## üë• Autores

| Nombre                           | Correo                                                    |
| -------------------------------- | --------------------------------------------------------- |
| **Brayan David Zuluaga Giraldo** | [bdzuluagag@eafit.edu.co](mailto:bdzuluagag@eafit.edu.co) |
| **Sof√≠a Mendieta Mar√≠n**         | [smendietam@eafit.edu.co](mailto:smendietam@eafit.edu.co) |

---

## üß© Licencia

Proyecto acad√©mico ‚Äî Universidad EAFIT (2025).
Uso educativo y demostrativo.

---
